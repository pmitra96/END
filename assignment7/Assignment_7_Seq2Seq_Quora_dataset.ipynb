{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Assignment 7 Seq2Seq Quora dataset.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMi_CpsUF7KF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2q9KBrKF-_O",
        "outputId": "5be45bb9-8f6f-4e09-b9e1-9afb270dfc3e"
      },
      "source": [
        "!pip install spacy --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.7/dist-packages (3.0.6)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.7.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7xC8oyVGD_v"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn9XMsK74MuH",
        "outputId": "dd84c977-cf3f-4739-d437-20c95a0d276f"
      },
      "source": [
        "cd drive/MyDrive/Assignment7/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/Assignment7/'\n",
            "/content/drive/MyDrive/Assignment7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxNRY39a2_1x",
        "outputId": "111c1b69-d05d-491a-9f00-cfd81439fd6a"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quora_duplicate_questions.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wz1sDTGBFvt"
      },
      "source": [
        "#mkdir Assignment7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlxTE82qBQP9",
        "outputId": "12ddefd3-e4ba-4a5d-f087-65e47154411f"
      },
      "source": [
        "%%bash\n",
        "cd Assignment7\n",
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "quora_duplicate_questions.tsv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "bash: line 1: cd: Assignment7: No such file or directory\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qmkx_tuBlJv"
      },
      "source": [
        "import pandas as pd\n",
        "quora_questions_df = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t', header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR3vpPJBCxmQ"
      },
      "source": [
        "questions_df = quora_questions_df[quora_questions_df['is_duplicate'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfwsK6g4DZW0",
        "outputId": "b7180e59-b18e-4fb4-9fa9-ddc91ceaa9ef"
      },
      "source": [
        "len(questions_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149263"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY2VIpVAF6VV",
        "outputId": "9e00cb2c-cf40-4caf-e496-158d28caeca7"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.0.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2021.5.30)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-20 15:30:00.447775: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOr-kK2QHQk4"
      },
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mp7IJcHHWbQ"
      },
      "source": [
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3N9abXMHY8Q"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWutMP63HkWF"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# train_df, test_df = train_test_split(questions_df, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "aDprUYfXMgXF",
        "outputId": "72fe2684-2435-4f11-a235-90212e02f571"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-193753be380d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3uScfleNO0L"
      },
      "source": [
        "train_df.question1.iloc[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxxAZreyMsZu"
      },
      "source": [
        "fields = [('src', SRC),('trg',TRG)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLnnzFAcnHbT"
      },
      "source": [
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8hIRN2hMXxG"
      },
      "source": [
        "train_examples = [data.Example.fromlist([questions_df.question1.iloc[i], questions_df.question2.iloc[i]], fields) for i in range(questions_df.shape[0])] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guA4Jcr8Pfxe"
      },
      "source": [
        "questionsDataset = data.Dataset(train_examples, fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuNKbUEfQdU9"
      },
      "source": [
        "(train_data, test_data) = questionsDataset.split(split_ratio=[0.7, 0.3], random_state=random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8J8GPa30anU",
        "outputId": "663aee6c-1214-47a2-9517-4e8e56d2c158"
      },
      "source": [
        "len(train_data), len(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104484, 44779)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEYqyz0_WZnq",
        "outputId": "c20c0b4e-14b4-40af-adab-554cd78d1aac"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['what', 'are', 'the', 'most', 'intellectually', 'stimulating', 'movies', 'you', 'have', 'ever', 'seen', '?'], 'trg': ['what', 'are', 'the', 'most', 'intellectually', 'stimulating', 'films', 'you', 'have', 'ever', 'watched', '?']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LPKh_zXQvO1"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_-YQcenIsu8",
        "outputId": "09a23fa2-9124-4aa7-9bf7-c7f48faaf097"
      },
      "source": [
        "print(f\"Unique tokens in source (en) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (en) vocabulary: 14513\n",
            "Unique tokens in target (en) vocabulary: 14455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cDck0MsVDd_"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4295hx_ILf-X"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data),\n",
        "    sort_key = lambda x: len(x.src),\n",
        "    sort_within_batch=True, \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Np8WQ0YVM9X"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZoP1hKWVPYn"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bywi1t6VRzf"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiHVnQHnVVDB"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY08jNRUVqkq",
        "outputId": "24ce2e9f-31c6-43bf-c206-b3d6497a2170"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(14513, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(14455, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=14455, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQzHd466VtPs",
        "outputId": "5cc091be-3ffd-49d5-dc61-fed1ec86934a"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 22,187,639 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-IrjoD4Vvl7"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaAoIL3fVxWr"
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj3JNXR_V1K8"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        #print(\"batch_number: \", i)\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL_HdGlYV21N"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W88n1Kw7V4W1"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkZRro5kV6XF",
        "outputId": "8722b7bd-6100-4e29-9bc9-d77a73d685f7"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {test_loss:.3f} |  Val. PPL: {math.exp(test_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_number:  0\n",
            "batch_number:  1\n",
            "batch_number:  2\n",
            "batch_number:  3\n",
            "batch_number:  4\n",
            "batch_number:  5\n",
            "batch_number:  6\n",
            "batch_number:  7\n",
            "batch_number:  8\n",
            "batch_number:  9\n",
            "batch_number:  10\n",
            "batch_number:  11\n",
            "batch_number:  12\n",
            "batch_number:  13\n",
            "batch_number:  14\n",
            "batch_number:  15\n",
            "batch_number:  16\n",
            "batch_number:  17\n",
            "batch_number:  18\n",
            "batch_number:  19\n",
            "batch_number:  20\n",
            "batch_number:  21\n",
            "batch_number:  22\n",
            "batch_number:  23\n",
            "batch_number:  24\n",
            "batch_number:  25\n",
            "batch_number:  26\n",
            "batch_number:  27\n",
            "batch_number:  28\n",
            "batch_number:  29\n",
            "batch_number:  30\n",
            "batch_number:  31\n",
            "batch_number:  32\n",
            "batch_number:  33\n",
            "batch_number:  34\n",
            "batch_number:  35\n",
            "batch_number:  36\n",
            "batch_number:  37\n",
            "batch_number:  38\n",
            "batch_number:  39\n",
            "batch_number:  40\n",
            "batch_number:  41\n",
            "batch_number:  42\n",
            "batch_number:  43\n",
            "batch_number:  44\n",
            "batch_number:  45\n",
            "batch_number:  46\n",
            "batch_number:  47\n",
            "batch_number:  48\n",
            "batch_number:  49\n",
            "batch_number:  50\n",
            "batch_number:  51\n",
            "batch_number:  52\n",
            "batch_number:  53\n",
            "batch_number:  54\n",
            "batch_number:  55\n",
            "batch_number:  56\n",
            "batch_number:  57\n",
            "batch_number:  58\n",
            "batch_number:  59\n",
            "batch_number:  60\n",
            "batch_number:  61\n",
            "batch_number:  62\n",
            "batch_number:  63\n",
            "batch_number:  64\n",
            "batch_number:  65\n",
            "batch_number:  66\n",
            "batch_number:  67\n",
            "batch_number:  68\n",
            "batch_number:  69\n",
            "batch_number:  70\n",
            "batch_number:  71\n",
            "batch_number:  72\n",
            "batch_number:  73\n",
            "batch_number:  74\n",
            "batch_number:  75\n",
            "batch_number:  76\n",
            "batch_number:  77\n",
            "batch_number:  78\n",
            "batch_number:  79\n",
            "batch_number:  80\n",
            "batch_number:  81\n",
            "batch_number:  82\n",
            "batch_number:  83\n",
            "batch_number:  84\n",
            "batch_number:  85\n",
            "batch_number:  86\n",
            "batch_number:  87\n",
            "batch_number:  88\n",
            "batch_number:  89\n",
            "batch_number:  90\n",
            "batch_number:  91\n",
            "batch_number:  92\n",
            "batch_number:  93\n",
            "batch_number:  94\n",
            "batch_number:  95\n",
            "batch_number:  96\n",
            "batch_number:  97\n",
            "batch_number:  98\n",
            "batch_number:  99\n",
            "batch_number:  100\n",
            "batch_number:  101\n",
            "batch_number:  102\n",
            "batch_number:  103\n",
            "batch_number:  104\n",
            "batch_number:  105\n",
            "batch_number:  106\n",
            "batch_number:  107\n",
            "batch_number:  108\n",
            "batch_number:  109\n",
            "batch_number:  110\n",
            "batch_number:  111\n",
            "batch_number:  112\n",
            "batch_number:  113\n",
            "batch_number:  114\n",
            "batch_number:  115\n",
            "batch_number:  116\n",
            "batch_number:  117\n",
            "batch_number:  118\n",
            "batch_number:  119\n",
            "batch_number:  120\n",
            "batch_number:  121\n",
            "batch_number:  122\n",
            "batch_number:  123\n",
            "batch_number:  124\n",
            "batch_number:  125\n",
            "batch_number:  126\n",
            "batch_number:  127\n",
            "batch_number:  128\n",
            "batch_number:  129\n",
            "batch_number:  130\n",
            "batch_number:  131\n",
            "batch_number:  132\n",
            "batch_number:  133\n",
            "batch_number:  134\n",
            "batch_number:  135\n",
            "batch_number:  136\n",
            "batch_number:  137\n",
            "batch_number:  138\n",
            "batch_number:  139\n",
            "batch_number:  140\n",
            "batch_number:  141\n",
            "batch_number:  142\n",
            "batch_number:  143\n",
            "batch_number:  144\n",
            "batch_number:  145\n",
            "batch_number:  146\n",
            "batch_number:  147\n",
            "batch_number:  148\n",
            "batch_number:  149\n",
            "batch_number:  150\n",
            "batch_number:  151\n",
            "batch_number:  152\n",
            "batch_number:  153\n",
            "batch_number:  154\n",
            "batch_number:  155\n",
            "batch_number:  156\n",
            "batch_number:  157\n",
            "batch_number:  158\n",
            "batch_number:  159\n",
            "batch_number:  160\n",
            "batch_number:  161\n",
            "batch_number:  162\n",
            "batch_number:  163\n",
            "batch_number:  164\n",
            "batch_number:  165\n",
            "batch_number:  166\n",
            "batch_number:  167\n",
            "batch_number:  168\n",
            "batch_number:  169\n",
            "batch_number:  170\n",
            "batch_number:  171\n",
            "batch_number:  172\n",
            "batch_number:  173\n",
            "batch_number:  174\n",
            "batch_number:  175\n",
            "batch_number:  176\n",
            "batch_number:  177\n",
            "batch_number:  178\n",
            "batch_number:  179\n",
            "batch_number:  180\n",
            "batch_number:  181\n",
            "batch_number:  182\n",
            "batch_number:  183\n",
            "batch_number:  184\n",
            "batch_number:  185\n",
            "batch_number:  186\n",
            "batch_number:  187\n",
            "batch_number:  188\n",
            "batch_number:  189\n",
            "batch_number:  190\n",
            "batch_number:  191\n",
            "batch_number:  192\n",
            "batch_number:  193\n",
            "batch_number:  194\n",
            "batch_number:  195\n",
            "batch_number:  196\n",
            "batch_number:  197\n",
            "batch_number:  198\n",
            "batch_number:  199\n",
            "batch_number:  200\n",
            "batch_number:  201\n",
            "batch_number:  202\n",
            "batch_number:  203\n",
            "batch_number:  204\n",
            "batch_number:  205\n",
            "batch_number:  206\n",
            "batch_number:  207\n",
            "batch_number:  208\n",
            "batch_number:  209\n",
            "batch_number:  210\n",
            "batch_number:  211\n",
            "batch_number:  212\n",
            "batch_number:  213\n",
            "batch_number:  214\n",
            "batch_number:  215\n",
            "batch_number:  216\n",
            "batch_number:  217\n",
            "batch_number:  218\n",
            "batch_number:  219\n",
            "batch_number:  220\n",
            "batch_number:  221\n",
            "batch_number:  222\n",
            "batch_number:  223\n",
            "batch_number:  224\n",
            "batch_number:  225\n",
            "batch_number:  226\n",
            "batch_number:  227\n",
            "batch_number:  228\n",
            "batch_number:  229\n",
            "batch_number:  230\n",
            "batch_number:  231\n",
            "batch_number:  232\n",
            "batch_number:  233\n",
            "batch_number:  234\n",
            "batch_number:  235\n",
            "batch_number:  236\n",
            "batch_number:  237\n",
            "batch_number:  238\n",
            "batch_number:  239\n",
            "batch_number:  240\n",
            "batch_number:  241\n",
            "batch_number:  242\n",
            "batch_number:  243\n",
            "batch_number:  244\n",
            "batch_number:  245\n",
            "batch_number:  246\n",
            "batch_number:  247\n",
            "batch_number:  248\n",
            "batch_number:  249\n",
            "batch_number:  250\n",
            "batch_number:  251\n",
            "batch_number:  252\n",
            "batch_number:  253\n",
            "batch_number:  254\n",
            "batch_number:  255\n",
            "batch_number:  256\n",
            "batch_number:  257\n",
            "batch_number:  258\n",
            "batch_number:  259\n",
            "batch_number:  260\n",
            "batch_number:  261\n",
            "batch_number:  262\n",
            "batch_number:  263\n",
            "batch_number:  264\n",
            "batch_number:  265\n",
            "batch_number:  266\n",
            "batch_number:  267\n",
            "batch_number:  268\n",
            "batch_number:  269\n",
            "batch_number:  270\n",
            "batch_number:  271\n",
            "batch_number:  272\n",
            "batch_number:  273\n",
            "batch_number:  274\n",
            "batch_number:  275\n",
            "batch_number:  276\n",
            "batch_number:  277\n",
            "batch_number:  278\n",
            "batch_number:  279\n",
            "batch_number:  280\n",
            "batch_number:  281\n",
            "batch_number:  282\n",
            "batch_number:  283\n",
            "batch_number:  284\n",
            "batch_number:  285\n",
            "batch_number:  286\n",
            "batch_number:  287\n",
            "batch_number:  288\n",
            "batch_number:  289\n",
            "batch_number:  290\n",
            "batch_number:  291\n",
            "batch_number:  292\n",
            "batch_number:  293\n",
            "batch_number:  294\n",
            "batch_number:  295\n",
            "batch_number:  296\n",
            "batch_number:  297\n",
            "batch_number:  298\n",
            "batch_number:  299\n",
            "batch_number:  300\n",
            "batch_number:  301\n",
            "batch_number:  302\n",
            "batch_number:  303\n",
            "batch_number:  304\n",
            "batch_number:  305\n",
            "batch_number:  306\n",
            "batch_number:  307\n",
            "batch_number:  308\n",
            "batch_number:  309\n",
            "batch_number:  310\n",
            "batch_number:  311\n",
            "batch_number:  312\n",
            "batch_number:  313\n",
            "batch_number:  314\n",
            "batch_number:  315\n",
            "batch_number:  316\n",
            "batch_number:  317\n",
            "batch_number:  318\n",
            "batch_number:  319\n",
            "batch_number:  320\n",
            "batch_number:  321\n",
            "batch_number:  322\n",
            "batch_number:  323\n",
            "batch_number:  324\n",
            "batch_number:  325\n",
            "batch_number:  326\n",
            "batch_number:  327\n",
            "batch_number:  328\n",
            "batch_number:  329\n",
            "batch_number:  330\n",
            "batch_number:  331\n",
            "batch_number:  332\n",
            "batch_number:  333\n",
            "batch_number:  334\n",
            "batch_number:  335\n",
            "batch_number:  336\n",
            "batch_number:  337\n",
            "batch_number:  338\n",
            "batch_number:  339\n",
            "batch_number:  340\n",
            "batch_number:  341\n",
            "batch_number:  342\n",
            "batch_number:  343\n",
            "batch_number:  344\n",
            "batch_number:  345\n",
            "batch_number:  346\n",
            "batch_number:  347\n",
            "batch_number:  348\n",
            "batch_number:  349\n",
            "batch_number:  350\n",
            "batch_number:  351\n",
            "batch_number:  352\n",
            "batch_number:  353\n",
            "batch_number:  354\n",
            "batch_number:  355\n",
            "batch_number:  356\n",
            "batch_number:  357\n",
            "batch_number:  358\n",
            "batch_number:  359\n",
            "batch_number:  360\n",
            "batch_number:  361\n",
            "batch_number:  362\n",
            "batch_number:  363\n",
            "batch_number:  364\n",
            "batch_number:  365\n",
            "batch_number:  366\n",
            "batch_number:  367\n",
            "batch_number:  368\n",
            "batch_number:  369\n",
            "batch_number:  370\n",
            "batch_number:  371\n",
            "batch_number:  372\n",
            "batch_number:  373\n",
            "batch_number:  374\n",
            "batch_number:  375\n",
            "batch_number:  376\n",
            "batch_number:  377\n",
            "batch_number:  378\n",
            "batch_number:  379\n",
            "batch_number:  380\n",
            "batch_number:  381\n",
            "batch_number:  382\n",
            "batch_number:  383\n",
            "batch_number:  384\n",
            "batch_number:  385\n",
            "batch_number:  386\n",
            "batch_number:  387\n",
            "batch_number:  388\n",
            "batch_number:  389\n",
            "batch_number:  390\n",
            "batch_number:  391\n",
            "batch_number:  392\n",
            "batch_number:  393\n",
            "batch_number:  394\n",
            "batch_number:  395\n",
            "batch_number:  396\n",
            "batch_number:  397\n",
            "batch_number:  398\n",
            "batch_number:  399\n",
            "batch_number:  400\n",
            "batch_number:  401\n",
            "batch_number:  402\n",
            "batch_number:  403\n",
            "batch_number:  404\n",
            "batch_number:  405\n",
            "batch_number:  406\n",
            "batch_number:  407\n",
            "batch_number:  408\n",
            "batch_number:  409\n",
            "batch_number:  410\n",
            "batch_number:  411\n",
            "batch_number:  412\n",
            "batch_number:  413\n",
            "batch_number:  414\n",
            "batch_number:  415\n",
            "batch_number:  416\n",
            "batch_number:  417\n",
            "batch_number:  418\n",
            "batch_number:  419\n",
            "batch_number:  420\n",
            "batch_number:  421\n",
            "batch_number:  422\n",
            "batch_number:  423\n",
            "batch_number:  424\n",
            "batch_number:  425\n",
            "batch_number:  426\n",
            "batch_number:  427\n",
            "batch_number:  428\n",
            "batch_number:  429\n",
            "batch_number:  430\n",
            "batch_number:  431\n",
            "batch_number:  432\n",
            "batch_number:  433\n",
            "batch_number:  434\n",
            "batch_number:  435\n",
            "batch_number:  436\n",
            "batch_number:  437\n",
            "batch_number:  438\n",
            "batch_number:  439\n",
            "batch_number:  440\n",
            "batch_number:  441\n",
            "batch_number:  442\n",
            "batch_number:  443\n",
            "batch_number:  444\n",
            "batch_number:  445\n",
            "batch_number:  446\n",
            "batch_number:  447\n",
            "batch_number:  448\n",
            "batch_number:  449\n",
            "batch_number:  450\n",
            "batch_number:  451\n",
            "batch_number:  452\n",
            "batch_number:  453\n",
            "batch_number:  454\n",
            "batch_number:  455\n",
            "batch_number:  456\n",
            "batch_number:  457\n",
            "batch_number:  458\n",
            "batch_number:  459\n",
            "batch_number:  460\n",
            "batch_number:  461\n",
            "batch_number:  462\n",
            "batch_number:  463\n",
            "batch_number:  464\n",
            "batch_number:  465\n",
            "batch_number:  466\n",
            "batch_number:  467\n",
            "batch_number:  468\n",
            "batch_number:  469\n",
            "batch_number:  470\n",
            "batch_number:  471\n",
            "batch_number:  472\n",
            "batch_number:  473\n",
            "batch_number:  474\n",
            "batch_number:  475\n",
            "batch_number:  476\n",
            "batch_number:  477\n",
            "batch_number:  478\n",
            "batch_number:  479\n",
            "batch_number:  480\n",
            "batch_number:  481\n",
            "batch_number:  482\n",
            "batch_number:  483\n",
            "batch_number:  484\n",
            "batch_number:  485\n",
            "batch_number:  486\n",
            "batch_number:  487\n",
            "batch_number:  488\n",
            "batch_number:  489\n",
            "batch_number:  490\n",
            "batch_number:  491\n",
            "batch_number:  492\n",
            "batch_number:  493\n",
            "batch_number:  494\n",
            "batch_number:  495\n",
            "batch_number:  496\n",
            "batch_number:  497\n",
            "batch_number:  498\n",
            "batch_number:  499\n",
            "batch_number:  500\n",
            "batch_number:  501\n",
            "batch_number:  502\n",
            "batch_number:  503\n",
            "batch_number:  504\n",
            "batch_number:  505\n",
            "batch_number:  506\n",
            "batch_number:  507\n",
            "batch_number:  508\n",
            "batch_number:  509\n",
            "batch_number:  510\n",
            "batch_number:  511\n",
            "batch_number:  512\n",
            "batch_number:  513\n",
            "batch_number:  514\n",
            "batch_number:  515\n",
            "batch_number:  516\n",
            "batch_number:  517\n",
            "batch_number:  518\n",
            "batch_number:  519\n",
            "batch_number:  520\n",
            "batch_number:  521\n",
            "batch_number:  522\n",
            "batch_number:  523\n",
            "batch_number:  524\n",
            "batch_number:  525\n",
            "batch_number:  526\n",
            "batch_number:  527\n",
            "batch_number:  528\n",
            "batch_number:  529\n",
            "batch_number:  530\n",
            "batch_number:  531\n",
            "batch_number:  532\n",
            "batch_number:  533\n",
            "batch_number:  534\n",
            "batch_number:  535\n",
            "batch_number:  536\n",
            "batch_number:  537\n",
            "batch_number:  538\n",
            "batch_number:  539\n",
            "batch_number:  540\n",
            "batch_number:  541\n",
            "batch_number:  542\n",
            "batch_number:  543\n",
            "batch_number:  544\n",
            "batch_number:  545\n",
            "batch_number:  546\n",
            "batch_number:  547\n",
            "batch_number:  548\n",
            "batch_number:  549\n",
            "batch_number:  550\n",
            "batch_number:  551\n",
            "batch_number:  552\n",
            "batch_number:  553\n",
            "batch_number:  554\n",
            "batch_number:  555\n",
            "batch_number:  556\n",
            "batch_number:  557\n",
            "batch_number:  558\n",
            "batch_number:  559\n",
            "batch_number:  560\n",
            "batch_number:  561\n",
            "batch_number:  562\n",
            "batch_number:  563\n",
            "batch_number:  564\n",
            "batch_number:  565\n",
            "batch_number:  566\n",
            "batch_number:  567\n",
            "batch_number:  568\n",
            "batch_number:  569\n",
            "batch_number:  570\n",
            "batch_number:  571\n",
            "batch_number:  572\n",
            "batch_number:  573\n",
            "batch_number:  574\n",
            "batch_number:  575\n",
            "batch_number:  576\n",
            "batch_number:  577\n",
            "batch_number:  578\n",
            "batch_number:  579\n",
            "batch_number:  580\n",
            "batch_number:  581\n",
            "batch_number:  582\n",
            "batch_number:  583\n",
            "batch_number:  584\n",
            "batch_number:  585\n",
            "batch_number:  586\n",
            "batch_number:  587\n",
            "batch_number:  588\n",
            "batch_number:  589\n",
            "batch_number:  590\n",
            "batch_number:  591\n",
            "batch_number:  592\n",
            "batch_number:  593\n",
            "batch_number:  594\n",
            "batch_number:  595\n",
            "batch_number:  596\n",
            "batch_number:  597\n",
            "batch_number:  598\n",
            "batch_number:  599\n",
            "batch_number:  600\n",
            "batch_number:  601\n",
            "batch_number:  602\n",
            "batch_number:  603\n",
            "batch_number:  604\n",
            "batch_number:  605\n",
            "batch_number:  606\n",
            "batch_number:  607\n",
            "batch_number:  608\n",
            "batch_number:  609\n",
            "batch_number:  610\n",
            "batch_number:  611\n",
            "batch_number:  612\n",
            "batch_number:  613\n",
            "batch_number:  614\n",
            "batch_number:  615\n",
            "batch_number:  616\n",
            "batch_number:  617\n",
            "batch_number:  618\n",
            "batch_number:  619\n",
            "batch_number:  620\n",
            "batch_number:  621\n",
            "batch_number:  622\n",
            "batch_number:  623\n",
            "batch_number:  624\n",
            "batch_number:  625\n",
            "batch_number:  626\n",
            "batch_number:  627\n",
            "batch_number:  628\n",
            "batch_number:  629\n",
            "batch_number:  630\n",
            "batch_number:  631\n",
            "batch_number:  632\n",
            "batch_number:  633\n",
            "batch_number:  634\n",
            "batch_number:  635\n",
            "batch_number:  636\n",
            "batch_number:  637\n",
            "batch_number:  638\n",
            "batch_number:  639\n",
            "batch_number:  640\n",
            "batch_number:  641\n",
            "batch_number:  642\n",
            "batch_number:  643\n",
            "batch_number:  644\n",
            "batch_number:  645\n",
            "batch_number:  646\n",
            "batch_number:  647\n",
            "batch_number:  648\n",
            "batch_number:  649\n",
            "batch_number:  650\n",
            "batch_number:  651\n",
            "batch_number:  652\n",
            "batch_number:  653\n",
            "batch_number:  654\n",
            "batch_number:  655\n",
            "batch_number:  656\n",
            "batch_number:  657\n",
            "batch_number:  658\n",
            "batch_number:  659\n",
            "batch_number:  660\n",
            "batch_number:  661\n",
            "batch_number:  662\n",
            "batch_number:  663\n",
            "batch_number:  664\n",
            "batch_number:  665\n",
            "batch_number:  666\n",
            "batch_number:  667\n",
            "batch_number:  668\n",
            "batch_number:  669\n",
            "batch_number:  670\n",
            "batch_number:  671\n",
            "batch_number:  672\n",
            "batch_number:  673\n",
            "batch_number:  674\n",
            "batch_number:  675\n",
            "batch_number:  676\n",
            "batch_number:  677\n",
            "batch_number:  678\n",
            "batch_number:  679\n",
            "batch_number:  680\n",
            "batch_number:  681\n",
            "batch_number:  682\n",
            "batch_number:  683\n",
            "batch_number:  684\n",
            "batch_number:  685\n",
            "batch_number:  686\n",
            "batch_number:  687\n",
            "batch_number:  688\n",
            "batch_number:  689\n",
            "batch_number:  690\n",
            "batch_number:  691\n",
            "batch_number:  692\n",
            "batch_number:  693\n",
            "batch_number:  694\n",
            "batch_number:  695\n",
            "batch_number:  696\n",
            "batch_number:  697\n",
            "batch_number:  698\n",
            "batch_number:  699\n",
            "batch_number:  700\n",
            "batch_number:  701\n",
            "batch_number:  702\n",
            "batch_number:  703\n",
            "batch_number:  704\n",
            "batch_number:  705\n",
            "batch_number:  706\n",
            "batch_number:  707\n",
            "batch_number:  708\n",
            "batch_number:  709\n",
            "batch_number:  710\n",
            "batch_number:  711\n",
            "batch_number:  712\n",
            "batch_number:  713\n",
            "batch_number:  714\n",
            "batch_number:  715\n",
            "batch_number:  716\n",
            "batch_number:  717\n",
            "batch_number:  718\n",
            "batch_number:  719\n",
            "batch_number:  720\n",
            "batch_number:  721\n",
            "batch_number:  722\n",
            "batch_number:  723\n",
            "batch_number:  724\n",
            "batch_number:  725\n",
            "batch_number:  726\n",
            "batch_number:  727\n",
            "batch_number:  728\n",
            "batch_number:  729\n",
            "batch_number:  730\n",
            "batch_number:  731\n",
            "batch_number:  732\n",
            "batch_number:  733\n",
            "batch_number:  734\n",
            "batch_number:  735\n",
            "batch_number:  736\n",
            "batch_number:  737\n",
            "batch_number:  738\n",
            "batch_number:  739\n",
            "batch_number:  740\n",
            "batch_number:  741\n",
            "batch_number:  742\n",
            "batch_number:  743\n",
            "batch_number:  744\n",
            "batch_number:  745\n",
            "batch_number:  746\n",
            "batch_number:  747\n",
            "batch_number:  748\n",
            "batch_number:  749\n",
            "batch_number:  750\n",
            "batch_number:  751\n",
            "batch_number:  752\n",
            "batch_number:  753\n",
            "batch_number:  754\n",
            "batch_number:  755\n",
            "batch_number:  756\n",
            "batch_number:  757\n",
            "batch_number:  758\n",
            "batch_number:  759\n",
            "batch_number:  760\n",
            "batch_number:  761\n",
            "batch_number:  762\n",
            "batch_number:  763\n",
            "batch_number:  764\n",
            "batch_number:  765\n",
            "batch_number:  766\n",
            "batch_number:  767\n",
            "batch_number:  768\n",
            "batch_number:  769\n",
            "batch_number:  770\n",
            "batch_number:  771\n",
            "batch_number:  772\n",
            "batch_number:  773\n",
            "batch_number:  774\n",
            "batch_number:  775\n",
            "batch_number:  776\n",
            "batch_number:  777\n",
            "batch_number:  778\n",
            "batch_number:  779\n",
            "batch_number:  780\n",
            "batch_number:  781\n",
            "batch_number:  782\n",
            "batch_number:  783\n",
            "batch_number:  784\n",
            "batch_number:  785\n",
            "batch_number:  786\n",
            "batch_number:  787\n",
            "batch_number:  788\n",
            "batch_number:  789\n",
            "batch_number:  790\n",
            "batch_number:  791\n",
            "batch_number:  792\n",
            "batch_number:  793\n",
            "batch_number:  794\n",
            "batch_number:  795\n",
            "batch_number:  796\n",
            "batch_number:  797\n",
            "batch_number:  798\n",
            "batch_number:  799\n",
            "batch_number:  800\n",
            "batch_number:  801\n",
            "batch_number:  802\n",
            "batch_number:  803\n",
            "batch_number:  804\n",
            "batch_number:  805\n",
            "batch_number:  806\n",
            "batch_number:  807\n",
            "batch_number:  808\n",
            "batch_number:  809\n",
            "batch_number:  810\n",
            "batch_number:  811\n",
            "batch_number:  812\n",
            "batch_number:  813\n",
            "batch_number:  814\n",
            "batch_number:  815\n",
            "batch_number:  816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-3c6802c6222c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'valid_iterator' is not defined"
          ]
        }
      ]
    }
  ]
}